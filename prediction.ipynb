{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 117 unique project URLs and saved to unique_urls.csv\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicate_links(input_csv, output_csv):\n",
    "    # 读取原始 CSV 文件\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # 提取 project_a 和 project_b 列的所有链接\n",
    "    links_a = df['project_a'].tolist()\n",
    "    links_b = df['project_b'].tolist()\n",
    "\n",
    "    # 合并所有链接，去重\n",
    "    all_links = list(set(links_a + links_b))\n",
    "\n",
    "    # 将结果保存为新的 CSV 文件\n",
    "    result_df = pd.DataFrame(all_links, columns=['project_url'])\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Processed {len(all_links)} unique project URLs and saved to {output_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例使用\n",
    "input_file = 'all.csv'  # 输入文件路径\n",
    "output_file = 'urls_unique.csv'  # 输出文件路径\n",
    "remove_duplicate_links(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 GitHub 项目特征（stars、forks、open issues、open pull requests）\n",
    "def get_github_features(project_url, headers):\n",
    "    \"\"\"\n",
    "    获取 GitHub 项目的特征：stars、forks、open issues 和 open pull requests\n",
    "    返回：stars, forks, open_issues, open_pulls\n",
    "    \"\"\"\n",
    "    repo_name = project_url.split('/')[-1]\n",
    "    owner_name = project_url.split('/')[-2]\n",
    "\n",
    "    # GitHub API 请求 URL\n",
    "    repo_url = f\"https://api.github.com/repos/{owner_name}/{repo_name}\"\n",
    "    issues_url = f\"https://api.github.com/repos/{owner_name}/{repo_name}/issues?state=open\"\n",
    "    pulls_url = f\"https://api.github.com/repos/{owner_name}/{repo_name}/pulls?state=open\"\n",
    "\n",
    "    # 请求项目的基本信息\n",
    "    response = requests.get(repo_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None, None, None, None  # 请求失败时返回 None\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    stars = data.get('stargazers_count', 0)  # 获取 stars 数量\n",
    "    forks = data.get('forks_count', 0)      # 获取 forks 数量\n",
    "\n",
    "    # 请求 open issues 和 open pull requests\n",
    "    open_issues_response = requests.get(issues_url, headers=headers)\n",
    "    open_pulls_response = requests.get(pulls_url, headers=headers)\n",
    "\n",
    "    if open_issues_response.status_code != 200 or open_pulls_response.status_code != 200:\n",
    "        return stars, forks, None, None  # 获取 issues 和 pull requests 失败时返回 None\n",
    "\n",
    "    open_issues = len(open_issues_response.json())  # 获取 open issues 数量\n",
    "    open_pulls = len(open_pulls_response.json())    # 获取 open pull requests 数量\n",
    "\n",
    "    return stars, forks, open_issues, open_pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 urls_unique 中的项目，获取其 GitHub 信息并保存到新的 CSV 文件\n",
    "def update_github_info(input_csv, output_csv, access_token):\n",
    "    \"\"\"\n",
    "    更新 CSV 数据，添加 stars、forks、open issues 和 open pull requests 信息\n",
    "    \"\"\"\n",
    "    # 读取原始数据\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 设置 GitHub 请求头\n",
    "    headers = {'Authorization': f'token {access_token}'}\n",
    "\n",
    "    # 初始化新的列\n",
    "    stars = []\n",
    "    forks = []\n",
    "    open_issues = []\n",
    "    open_pulls = []\n",
    "\n",
    "    # 遍历每个项目 URL，获取其 GitHub 信息\n",
    "    for i, row in df.iterrows():\n",
    "        project_url = row['project_url']  # 获取 project_url\n",
    "\n",
    "        # 获取 GitHub 项目的特征\n",
    "        stars_val, forks_val, open_issues_val, open_pulls_val = get_github_features(project_url, headers)\n",
    "\n",
    "        # 将结果添加到列表中\n",
    "        stars.append(stars_val)\n",
    "        forks.append(forks_val)\n",
    "        open_issues.append(open_issues_val)\n",
    "        open_pulls.append(open_pulls_val)\n",
    "\n",
    "        # 每处理完一行就保存一次\n",
    "        df.at[i, 'stars'] = stars_val\n",
    "        df.at[i, 'forks'] = forks_val\n",
    "        df.at[i, 'open_issues'] = open_issues_val\n",
    "        df.at[i, 'open_pulls'] = open_pulls_val\n",
    "\n",
    "        # 每行数据处理完后保存到输出文件\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Processed row {i + 1} and saved to {output_csv}\")\n",
    "\n",
    "    print(f\"Finished processing. Data saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 1 and saved to urls_cache.csv\n",
      "Processed row 2 and saved to urls_cache.csv\n",
      "Processed row 3 and saved to urls_cache.csv\n",
      "Processed row 4 and saved to urls_cache.csv\n",
      "Processed row 5 and saved to urls_cache.csv\n",
      "Processed row 6 and saved to urls_cache.csv\n",
      "Processed row 7 and saved to urls_cache.csv\n",
      "Processed row 8 and saved to urls_cache.csv\n",
      "Processed row 9 and saved to urls_cache.csv\n",
      "Processed row 10 and saved to urls_cache.csv\n",
      "Processed row 11 and saved to urls_cache.csv\n",
      "Processed row 12 and saved to urls_cache.csv\n",
      "Processed row 13 and saved to urls_cache.csv\n",
      "Processed row 14 and saved to urls_cache.csv\n",
      "Processed row 15 and saved to urls_cache.csv\n",
      "Processed row 16 and saved to urls_cache.csv\n",
      "Processed row 17 and saved to urls_cache.csv\n",
      "Processed row 18 and saved to urls_cache.csv\n",
      "Processed row 19 and saved to urls_cache.csv\n",
      "Processed row 20 and saved to urls_cache.csv\n",
      "Processed row 21 and saved to urls_cache.csv\n",
      "Processed row 22 and saved to urls_cache.csv\n",
      "Processed row 23 and saved to urls_cache.csv\n",
      "Processed row 24 and saved to urls_cache.csv\n",
      "Processed row 25 and saved to urls_cache.csv\n",
      "Processed row 26 and saved to urls_cache.csv\n",
      "Processed row 27 and saved to urls_cache.csv\n",
      "Processed row 28 and saved to urls_cache.csv\n",
      "Processed row 29 and saved to urls_cache.csv\n",
      "Processed row 30 and saved to urls_cache.csv\n",
      "Processed row 31 and saved to urls_cache.csv\n",
      "Processed row 32 and saved to urls_cache.csv\n",
      "Processed row 33 and saved to urls_cache.csv\n",
      "Processed row 34 and saved to urls_cache.csv\n",
      "Processed row 35 and saved to urls_cache.csv\n",
      "Processed row 36 and saved to urls_cache.csv\n",
      "Processed row 37 and saved to urls_cache.csv\n",
      "Processed row 38 and saved to urls_cache.csv\n",
      "Processed row 39 and saved to urls_cache.csv\n",
      "Processed row 40 and saved to urls_cache.csv\n",
      "Processed row 41 and saved to urls_cache.csv\n",
      "Processed row 42 and saved to urls_cache.csv\n",
      "Processed row 43 and saved to urls_cache.csv\n",
      "Processed row 44 and saved to urls_cache.csv\n",
      "Processed row 45 and saved to urls_cache.csv\n",
      "Processed row 46 and saved to urls_cache.csv\n",
      "Processed row 47 and saved to urls_cache.csv\n",
      "Processed row 48 and saved to urls_cache.csv\n",
      "Processed row 49 and saved to urls_cache.csv\n",
      "Processed row 50 and saved to urls_cache.csv\n",
      "Processed row 51 and saved to urls_cache.csv\n",
      "Processed row 52 and saved to urls_cache.csv\n",
      "Processed row 53 and saved to urls_cache.csv\n",
      "Processed row 54 and saved to urls_cache.csv\n",
      "Processed row 55 and saved to urls_cache.csv\n",
      "Processed row 56 and saved to urls_cache.csv\n",
      "Processed row 57 and saved to urls_cache.csv\n",
      "Processed row 58 and saved to urls_cache.csv\n",
      "Processed row 59 and saved to urls_cache.csv\n",
      "Processed row 60 and saved to urls_cache.csv\n",
      "Processed row 61 and saved to urls_cache.csv\n",
      "Processed row 62 and saved to urls_cache.csv\n",
      "Processed row 63 and saved to urls_cache.csv\n",
      "Processed row 64 and saved to urls_cache.csv\n",
      "Processed row 65 and saved to urls_cache.csv\n",
      "Processed row 66 and saved to urls_cache.csv\n",
      "Processed row 67 and saved to urls_cache.csv\n",
      "Processed row 68 and saved to urls_cache.csv\n",
      "Processed row 69 and saved to urls_cache.csv\n",
      "Processed row 70 and saved to urls_cache.csv\n",
      "Processed row 71 and saved to urls_cache.csv\n",
      "Processed row 72 and saved to urls_cache.csv\n",
      "Processed row 73 and saved to urls_cache.csv\n",
      "Processed row 74 and saved to urls_cache.csv\n",
      "Processed row 75 and saved to urls_cache.csv\n",
      "Processed row 76 and saved to urls_cache.csv\n",
      "Processed row 77 and saved to urls_cache.csv\n",
      "Processed row 78 and saved to urls_cache.csv\n",
      "Processed row 79 and saved to urls_cache.csv\n",
      "Processed row 80 and saved to urls_cache.csv\n",
      "Processed row 81 and saved to urls_cache.csv\n",
      "Processed row 82 and saved to urls_cache.csv\n",
      "Processed row 83 and saved to urls_cache.csv\n",
      "Processed row 84 and saved to urls_cache.csv\n",
      "Processed row 85 and saved to urls_cache.csv\n",
      "Processed row 86 and saved to urls_cache.csv\n",
      "Processed row 87 and saved to urls_cache.csv\n",
      "Processed row 88 and saved to urls_cache.csv\n",
      "Processed row 89 and saved to urls_cache.csv\n",
      "Processed row 90 and saved to urls_cache.csv\n",
      "Processed row 91 and saved to urls_cache.csv\n",
      "Processed row 92 and saved to urls_cache.csv\n",
      "Processed row 93 and saved to urls_cache.csv\n",
      "Processed row 94 and saved to urls_cache.csv\n",
      "Processed row 95 and saved to urls_cache.csv\n",
      "Processed row 96 and saved to urls_cache.csv\n",
      "Processed row 97 and saved to urls_cache.csv\n",
      "Processed row 98 and saved to urls_cache.csv\n",
      "Processed row 99 and saved to urls_cache.csv\n",
      "Processed row 100 and saved to urls_cache.csv\n",
      "Processed row 101 and saved to urls_cache.csv\n",
      "Processed row 102 and saved to urls_cache.csv\n",
      "Processed row 103 and saved to urls_cache.csv\n",
      "Processed row 104 and saved to urls_cache.csv\n",
      "Processed row 105 and saved to urls_cache.csv\n",
      "Processed row 106 and saved to urls_cache.csv\n",
      "Processed row 107 and saved to urls_cache.csv\n",
      "Processed row 108 and saved to urls_cache.csv\n",
      "Processed row 109 and saved to urls_cache.csv\n",
      "Processed row 110 and saved to urls_cache.csv\n",
      "Processed row 111 and saved to urls_cache.csv\n",
      "Processed row 112 and saved to urls_cache.csv\n",
      "Processed row 113 and saved to urls_cache.csv\n",
      "Processed row 114 and saved to urls_cache.csv\n",
      "Processed row 115 and saved to urls_cache.csv\n",
      "Processed row 116 and saved to urls_cache.csv\n",
      "Processed row 117 and saved to urls_cache.csv\n",
      "Finished processing. Data saved to urls_cache.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = 'unique_urls.csv'  # 输入文件路径，包含 GitHub 项目 URL\n",
    "output_file = 'urls_cache.csv'  # 输出文件路径\n",
    "access_token = \"xxx\" # GitHub 访问令牌 需要自行设置\n",
    "update_github_info(input_file, output_file, access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数：根据 project_url 获取相关特征\n",
    "def get_project_features(dataset_file,project_url):\n",
    "    # 读取 dataset.csv\n",
    "    dataset_df = pd.read_csv(dataset_file)\n",
    "    # 读取 urls_cache.csv\n",
    "    cache_df = pd.read_csv(cache_file)\n",
    "    # 将 urls_cache.csv 转换为字典，方便根据 project_url 查找特征\n",
    "    cache_dict = cache_df.set_index('project_url').to_dict(orient='index')\n",
    "    if project_url in cache_dict:\n",
    "        return cache_dict[project_url]\n",
    "    else:\n",
    "        return None  # 如果没有找到该 URL 对应的信息，返回 None\n",
    "    \n",
    "def add_github_features(dataset_file, cache_file, output_file):\n",
    "    # 为 dataset.csv 添加新的列\n",
    "    stars_a, forks_a, open_issues_a, open_pulls_a = [], [], [], []\n",
    "    stars_b, forks_b, open_issues_b, open_pulls_b = [], [], [], []\n",
    "\n",
    "    # 读取 dataset.csv\n",
    "    dataset_df = pd.read_csv(dataset_file)\n",
    "    \n",
    "    # 遍历 dataset.csv 中的每一行，添加相应的特征\n",
    "    for _, row in dataset_df.iterrows():\n",
    "        # 获取 project_a 和 project_b 的特征\n",
    "        features_a = get_project_features(dataset_file, row['project_a'])\n",
    "        features_b = get_project_features(dataset_file, row['project_b'])\n",
    "\n",
    "        if features_a:\n",
    "            stars_a.append(features_a['stars'])\n",
    "            forks_a.append(features_a['forks'])\n",
    "            open_issues_a.append(features_a['open_issues'])\n",
    "            open_pulls_a.append(features_a['open_pulls'])\n",
    "        else:\n",
    "            stars_a.append(None)\n",
    "            forks_a.append(None)\n",
    "            open_issues_a.append(None)\n",
    "            open_pulls_a.append(None)\n",
    "\n",
    "        if features_b:\n",
    "            stars_b.append(features_b['stars'])\n",
    "            forks_b.append(features_b['forks'])\n",
    "            open_issues_b.append(features_b['open_issues'])\n",
    "            open_pulls_b.append(features_b['open_pulls'])\n",
    "        else:\n",
    "            stars_b.append(None)\n",
    "            forks_b.append(None)\n",
    "            open_issues_b.append(None)\n",
    "            open_pulls_b.append(None)\n",
    "\n",
    "    # 将新列添加到 dataset_df 中\n",
    "    dataset_df['stars_a'] = stars_a\n",
    "    dataset_df['forks_a'] = forks_a\n",
    "    dataset_df['open_issues_a'] = open_issues_a\n",
    "    dataset_df['open_pulls_a'] = open_pulls_a\n",
    "    dataset_df['stars_b'] = stars_b\n",
    "    dataset_df['forks_b'] = forks_b\n",
    "    dataset_df['open_issues_b'] = open_issues_b\n",
    "    dataset_df['open_pulls_b'] = open_pulls_b\n",
    "\n",
    "    # 将更新后的 DataFrame 保存到新的 CSV 文件\n",
    "    dataset_df.to_csv(output_file, index=False)\n",
    "    print(f\"Updated data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例使用\n",
    "dataset_file = 'dataset.csv'  # 输入文件路径，包含项目 URL 和权重\n",
    "cache_file = 'urls_cache.csv'  # 输入文件路径，包含 GitHub 项目的特征\n",
    "output_file = 'dataset_update.csv'  # 输出文件路径，保存更新后的数据\n",
    "add_github_features(dataset_file, cache_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 数据加载与预处理\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # 加载数据\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # id,project_a,project_b,weight_a,weight_b,stars_a,forks_a,open_issues_a,open_pulls_a,stars_b,forks_b,open_issues_b,open_pulls_b\n",
    "    \n",
    "    # 提取特征和目标\n",
    "    features = df[['stars_a', 'forks_a', 'open_issues_a', 'open_pulls_a', 'stars_b', 'forks_b', 'open_issues_b', 'open_pulls_b']].values\n",
    "    targets = df[['weight_a']].values\n",
    "\n",
    "    # 特征标准化\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    return features_scaled, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 构建神经网络模型\n",
    "class FundingPredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FundingPredictionModel, self).__init__()\n",
    "        # 定义神经网络结构\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.pool(F.relu(self.fc2(x)), 2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FundingPredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FundingPredictionModel, self).__init__()\n",
    "        # 定义神经网络结构\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(32 * (input_dim // 4), 128)  # 输入维度减半后\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)# ([1909, 1, 8])\n",
    "        x = self.conv1(x) # [1909, 16, 8]\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # [1909, 16, 4]\n",
    "        \n",
    "        x = self.conv2(x)# [1909, 32, 4]\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # [1909, 32, 2]\n",
    "        \n",
    "        x = x.view(x.size(0), -1)# [1909, 64]\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x)) # [1909, 128]\n",
    "        x = F.relu(self.fc2(x)) # [1909, 64]\n",
    "        x = F.relu(self.fc3(x)) # [1909, 32]\n",
    "        x = self.fc4(x) # [1909, 1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: 模型训练\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=32, lr=0.001):\n",
    "    # 损失函数与优化器\n",
    "    criterion = nn.MSELoss()  # 使用均方误差损失函数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 将数据转换为 Tensor\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    # 训练过程\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播\n",
    "        outputs = model(X_train_tensor)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每十分之一个epoch打印一次损失\n",
    "        if (epoch + 1) % (epochs//10) == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "            # 每10个epoch验证\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val_tensor)\n",
    "                val_loss = mean_squared_error(y_val_tensor.numpy(), val_outputs.numpy())\n",
    "                print(f\"Validation MSE at epoch {epoch+1}: {val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: 模型评估\n",
    "def evaluate_model(model, X_test):\n",
    "    model.eval()\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions  = model(X_test_tensor).squeeze()\n",
    "\n",
    "    submission = pd.DataFrame({'id': test_data['id'], 'pred': predictions.numpy()})\n",
    "    submission.to_csv('sample_submission.csv', index=False, float_format='%.11f')\n",
    "    print(\"Predictions saved to sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:cpu\n",
      "Input dimension: 8\n",
      "Epoch [60/600], Loss: 0.1155\n",
      "Validation MSE at epoch 60: 0.1180\n",
      "Epoch [120/600], Loss: 0.1098\n",
      "Validation MSE at epoch 120: 0.1114\n",
      "Epoch [180/600], Loss: 0.1018\n",
      "Validation MSE at epoch 180: 0.1056\n",
      "Epoch [240/600], Loss: 0.0953\n",
      "Validation MSE at epoch 240: 0.0999\n",
      "Epoch [300/600], Loss: 0.0893\n",
      "Validation MSE at epoch 300: 0.0945\n",
      "Epoch [360/600], Loss: 0.0841\n",
      "Validation MSE at epoch 360: 0.0896\n",
      "Epoch [420/600], Loss: 0.0795\n",
      "Validation MSE at epoch 420: 0.0873\n",
      "Epoch [480/600], Loss: 0.0766\n",
      "Validation MSE at epoch 480: 0.0852\n",
      "Epoch [540/600], Loss: 0.0733\n",
      "Validation MSE at epoch 540: 0.0825\n",
      "Epoch [600/600], Loss: 0.0682\n",
      "Validation MSE at epoch 600: 0.0823\n",
      "Evaluating model on test data...\n",
      "Predictions saved to sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "device= \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:3\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device:{device}\")\n",
    "\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Step 1: 数据加载与预处理\n",
    "file_path = 'dataset_update.csv'  # 数据集文件路径\n",
    "X, y = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Step 2: 划分数据集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: 模型初始化\n",
    "input_dim = X_train.shape[1]  # 特征数量\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "model = FundingPredictionModel(input_dim)\n",
    "\n",
    "# Step 4: 训练模型\n",
    "trained_model = train_model(model, X_train, y_train, X_val, y_val, epochs=600, batch_size=512)\n",
    "\n",
    "# Step 5: 评估模型\n",
    "# 读取测试数据\n",
    "test_data = pd.read_csv('test.csv')\n",
    "cache_file = 'urls_cache.csv'  # 输入文件路径，包含 GitHub 项目的特征\n",
    "test_data[['stars_a', 'forks_a', 'open_issues_a', 'open_pulls_a']] = test_data['project_a'].apply(lambda x: pd.Series(get_project_features(cache_file,x)))\n",
    "test_data[['stars_b', 'forks_b', 'open_issues_b', 'open_pulls_b']] = test_data['project_b'].apply(lambda x: pd.Series(get_project_features(cache_file,x)))\n",
    "\n",
    "# 选择特征\n",
    "features = ['stars_a', 'forks_a', 'open_issues_a', 'open_pulls_a', \n",
    "            'stars_b', 'forks_b', 'open_issues_b', 'open_pulls_b']\n",
    "# 标准化测试数据\n",
    "X_test = test_data[features].values\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "print(\"Evaluating model on test data...\")\n",
    "evaluate_model(trained_model, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      stars_a  forks_a  open_issues_a  open_pulls_a\n",
      "0       731.0     75.0           18.0           9.0\n",
      "1       731.0     75.0           18.0           9.0\n",
      "2       731.0     75.0           18.0           9.0\n",
      "3       731.0     75.0           18.0           9.0\n",
      "4      3499.0   1045.0           30.0          30.0\n",
      "...       ...      ...            ...           ...\n",
      "1018    181.0     32.0           30.0          14.0\n",
      "1019    181.0     32.0           30.0          14.0\n",
      "1020   1466.0    478.0           30.0          14.0\n",
      "1021   1466.0    478.0           30.0          14.0\n",
      "1022   3199.0   1166.0           30.0          30.0\n",
      "\n",
      "[1023 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 实例使用\n",
    "test_data = pd.read_csv('test.csv')\n",
    "cache_file = 'urls_cache.csv'  # 输入文件路径，包含 GitHub 项目的特征\n",
    "test_data[['stars_a', 'forks_a', 'open_issues_a', 'open_pulls_a']] = test_data['project_a'].apply(lambda x: pd.Series(get_project_features(cache_file, x)))\n",
    "print(test_data[['stars_a', 'forks_a', 'open_issues_a', 'open_pulls_a']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
